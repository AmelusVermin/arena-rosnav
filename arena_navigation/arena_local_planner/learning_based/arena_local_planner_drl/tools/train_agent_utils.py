import os
import datetime
import json
from typing import Union

from stable_baselines3 import PPO

""" dictionary holding the params for key validation when loading from json """
hyperparams = {
    key: None for key in [
        'agent_name', 'robot', 'gamma', 'n_steps', 'ent_coef', 'learning_rate', 'vf_coef', 'max_grad_norm', 'gae_lambda', 'batch_size', 
        'n_epochs', 'clip_range', 'reward_fnc', 'discrete_action_space', 'normalize', 'task_mode', 'curr_stage', 'n_timesteps', 
        'train_max_steps_per_episode', 'eval_max_steps_per_episode', 'goal_radius'
    ]
}

class agent_hyperparams(object):
    """ Class containing agent specific hyperparameters (for documentation and typing validation purposes)

    :param agent_name: Precise agent name (as generated by get_agent_name())
    :param robot: Robot name to load robot specific .yaml file containing settings
    :param gamma: Discount factor
    :param n_steps: The number of steps to run for each environment per update
    :param ent_coef: Entropy coefficient for the loss calculation
    :param learning_rate: The learning rate, it can be a function
        of the current progress remaining (from 1 to 0)
        (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)
    :param vf_coef: Value function coefficient for the loss calculation
    :param max_grad_norm: The maximum value for the gradient clipping
    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator
    :param batch_size: Minibatch size
    :param n_epochs: Number of epoch when optimizing the surrogate loss
    :param clip_range: Clipping parameter, it can be a function of the current progress
        remaining (from 1 to 0).
    :param train_max_steps_per_episode: Max timesteps per training episode
    :param eval_max_steps_per_episode: Max timesteps per evaluation episode
    :param goal_radius: Radius of the goal
    :param reward_fnc: Number of the reward function (defined in ../rl_agent/utils/reward.py)
    :param discrete_action_space: If robot uses discrete action space
    :param normalize: If observations are normalized before fed to the network
    :param task_mode: Mode tasks will be generated in (custom, random, staged).
    :param curr_stage: In case of staged training which stage to start with.
    :param n_timesteps: The number of timesteps trained on in total.
    """
    def __init__(self, agent_name: str, robot: str, gamma: float, n_steps: int, ent_coef: float, learning_rate: float, vf_coef: float, max_grad_norm: float, gae_lambda: float,
                 batch_size: int, n_epochs: int, clip_range: float, reward_fnc: str, discrete_action_space: bool, normalize: bool, task_mode: str, curr_stage: int = 0,
                 train_max_steps_per_episode: int = 150, eval_max_steps_per_episode: int = 150, goal_radius: float = 1.00, n_timesteps: int = 0):
        self.agent_name = agent_name
        self.robot = robot
        self.gamma = gamma 
        self.n_steps = n_steps
        self.ent_coef = ent_coef
        self.learning_rate = learning_rate
        self.vf_coef = vf_coef
        self.max_grad_norm = max_grad_norm
        self.gae_lambda = gae_lambda
        self.batch_size = batch_size
        self.n_epochs = n_epochs
        self.clip_range = clip_range
        self.train_max_steps_per_episode = train_max_steps_per_episode
        self.eval_max_steps_per_episode = eval_max_steps_per_episode
        self.goal_radius = goal_radius
        self.reward_fnc = reward_fnc
        self.discrete_action_space = discrete_action_space
        self.normalize = normalize
        self.task_mode = task_mode
        self.curr_stage = curr_stage
        self.n_timesteps = n_timesteps
        


def initialize_hyperparameters(agent_name: str, PATHS: dict, hyperparams_obj: agent_hyperparams, load_target: str):
    """
    Write hyperparameters to json file in case agent is new otherwise load existing hyperparameters

    :param agent_name: agent name to save to/load from file system
    :param PATHS: dictionary containing model specific paths
    :param hyperparams_obj(object, agent_hyperparams): object containing containing model specific hyperparameters
    """
    if load_target is None:
        write_hyperparameters_json(hyperparams_obj, PATHS)
    hyperparams = load_hyperparameters_json(PATHS=PATHS)
    print_hyperparameters(hyperparams)
    return hyperparams


def write_hyperparameters_json(hyperparams_obj: agent_hyperparams, PATHS: dict):
    """
    Write hyperparameters to json file

    :param hyperparams_obj(object, agent_hyperparams): object containing containing model specific hyperparameters
    :param PATHS: dictionary containing model specific paths
    """
    doc_location = os.path.join(PATHS.get('model'), "hyperparameters.json")

    with open(doc_location, "w", encoding='utf-8') as target:
        json.dump(hyperparams_obj.__dict__, target, ensure_ascii=False, indent=4)


def load_hyperparameters_json(PATHS: dict):
    """
    Load hyperparameters from json file

    :param PATHS: dictionary containing model specific paths
    """
    doc_location = os.path.join(PATHS.get('model'), "hyperparameters.json")

    if os.path.isfile(doc_location):
        with open(doc_location, "r") as file:
            hyperparams = json.load(file)
        check_hyperparam_format(loaded_hyperparams=hyperparams, PATHS=PATHS)
        return hyperparams
    else:
        raise FileNotFoundError("Found no 'hyperparameters.json' in %s" % PATHS.get('model'))


def update_total_timesteps_json(timesteps: int, PATHS:dict):
    """
    Update total number of timesteps in json file

    :param hyperparams_obj(object, agent_hyperparams): object containing containing model specific hyperparameters
    :param PATHS: dictionary containing model specific paths
    """
    doc_location = os.path.join(PATHS.get('model'), "hyperparameters.json")
    hyperparams = load_hyperparameters_json(PATHS=PATHS)
    
    try:
        curr_timesteps = int(hyperparams['n_timesteps']) + timesteps
        hyperparams['n_timesteps'] = curr_timesteps
    except Exception:
        raise Warning("Parameter 'total_timesteps' not found or not of type Integer in 'hyperparameter.json'!")
    else:
        with open(doc_location, "w", encoding='utf-8') as target:
            json.dump(hyperparams, target, ensure_ascii=False, indent=4)
    

def print_hyperparameters(hyperparams: Union[dict, agent_hyperparams]):
    print("\n--------------------------------")
    print("         HYPERPARAMETERS         \n")
    for param, param_val in hyperparams.items():
        print("{:30s}{:<10s}".format((param+":"), str(param_val)))
    print("--------------------------------\n\n")


def check_hyperparam_format(loaded_hyperparams: dict, PATHS: dict):
    if not set(hyperparams.keys()) == set(loaded_hyperparams.keys()):
        missing_keys = set(hyperparams.keys()).difference(set(loaded_hyperparams.keys()))
        raise AssertionError(f"'hyperparameters.json' in {PATHS.get('model')} has unmatching keys, following keys missing: {missing_keys}" )
    if not isinstance(loaded_hyperparams['discrete_action_space'], bool):
        raise TypeError("Parameter 'discrete_action_space' not of type bool")
    if not loaded_hyperparams['task_mode'] in ["custom", "random", "staged"]:
        raise TypeError("Parameter 'task_mode' has unknown value")


def update_hyperparam_model(model: PPO, params: dict, n_envs: int = 1):
    """
    Updates parameter of loaded PPO agent

    :param model(object, PPO): loaded PPO agent
    :param params: dictionary containing loaded hyperparams
    :param n_envs: number of parallel environments
    """
    if model.batch_size != params['batch_size']:
        model.batch_size = params['batch_size']
    if model.gamma != params['gamma']:
        model.gamma = params['gamma']
    if model.n_steps != params['n_steps']:
        model.n_steps = params['n_steps']
    if model.ent_coef != params['ent_coef']:
        model.ent_coef = params['ent_coef']
    if model.learning_rate != params['learning_rate']:
        model.learning_rate = params['learning_rate']
    if model.vf_coef != params['vf_coef']:
        model.vf_coef = params['vf_coef']
    if model.max_grad_norm != params['max_grad_norm']:
        model.max_grad_norm = params['max_grad_norm']
    if model.gae_lambda != params['gae_lambda']:
        model.gae_lambda = params['gae_lambda']
    if model.n_epochs != params['n_epochs']:
        model.n_epochs = params['n_epochs']
    """
    if model.clip_range != params['clip_range']:
        model.clip_range = params['clip_range']
    """
    if model.n_envs != n_envs:
        model.update_n_envs()
