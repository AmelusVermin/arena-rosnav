# important note when adding params! unique key names across all nested params are assumed. 
# This file gets flattened during parsing without adding prefixes to the key names
robot:
  ## settings related to the robot itself
  robot: "myrobot"
  continuous_actions:
    linear_range: [0, 0.3]
    angular_range: [-2.7, 2.7]
environment:
  ## settings related to the gym environment
  max_distance_goal: 100
  global_planner_class: "global_planners.ROSGlobalPlanner.ROSGlobalPlanner" #"global_planners.dummy_global.Dummy"
  mid_planner_class: "mid_planners.ignc_intermediate_planner.IntermediatePlanner" #"mid_planners.dummy_mid.Dummy"
  # default: "configs/training_curriculum.yaml"
  task_curriculum_path: "configs/training_curriculum.yaml"
  # the goal is reached when the robot enters the circle determined by this radius around the goal, the orientation is ignored
  goal_radius: 0.3075
  reward_fnc: "rule_05"
  # a tolerance value that is used for collision checks. If the robot gets closer to an obstacle than this tolerance, a collsion will be detected. scan.min <= robot_radius + collision_tolerance.
  collision_tolerance: 0 
  # task mode of task generator: "random" or "staged" 
  task_mode: "staged"
  task_curr_stage: 1
  safe_dist: null
  extended_eval: True
  train_max_steps_per_episode: 1024
  # unused, only set for compatability issues with hyperparameter.json
  eval_max_steps_per_episode: 64
  use_first_synced_obs: True
  max_deque_size: 10
  sync_slop: 0.05
  # interval in steps when the planners are called. 
  # 1 = every step, 2 = every 2 steps the planner is called and the global_plan/subgoal gets updated
  global_planner_call_interval: 1
  mid_planner_call_interval: 1
  #map_update_frequency: 20 
  map_type: "indoor"
  indoor_prob: 0.5

training:
  ## settings related to training and model
  #sub directory of this package
  train_log_dir: "logs/training_logs"
  tensorboard_log_dir: "logs/tensorboard"
  ## learning related settings
  batch_size: 128
  gamma: 0.99 
  ent_coef: 0.01 #0.005 
  learning_rate: 2.5e-4 #0.0003
  vf_coef: 0.5 #0.22
  max_grad_norm: 0.5
  gae_lambda: 0.95
  # number of envrionments run in parallel should be a multiple of mini_batch_size for recurrent policies
  mini_batch_size: 1
  n_epochs: 4
  clip_range: 0.22
  normalize: True
  total_timesteps: 524288
  train_verbose: 1
  seed: null
model:
  ## model related settings. Used in model_builder.py
  # boolean: true if an existing model shall be loaded. 
  # model_path and update_params need to be set if true
  load_model: false
  # string: path to zip file containing a model
  model_path: "/home/marvin/catkin_ws/src/arena-rosnav/arena_navigation/arena_local_planner/learning_based/arena_combined_planner_drl/models/trained/AGENT_8_2021_10_11__20_34/best_model.zip"
  # boolean: true if the saved model params shall be overwritten by the current config values or not
  # paths are definitly overwritten as they might not exist anymore
  overwrite_params: true
  # string: agent_type that shall be created like CUSTOM_MLP, or AGENT_6, use command line option --show_registered_types or -srt
  # is ignored if model_path is true
  agent_type: "CUSTOM_MLP_LN_LSTM" #"CUSTOM_MLP"
  # determines used observation space: available "FULL_LENGTH" (goal, subgoal, global plan length, scan), "FULL_PATH" (goal, subgoal, global plan path, scan), "MIN" (goal, scan), 
  #  "SUBGOAL" (goal, subgoal, scan), "GLOBAL_LENGTH" (goal, global plan length, scan), "GLOBAL_PATH" (goal, global plan path, scan)
  observation_space_type: "FULL_PATH" 
  num_global_plan_points: 50
  gp_point_skip_rate: 4

trainstage:
  ## settings related to creating TrainStage callback
  # type can be either 'succ' or 'rew'
  stage_threshold_type: "succ"
  stage_upper_thres: 0.001
  stage_lower_thres: 0.0
  stage_verbose: 1
evaluation:
  ## settings related to creating evaluation callback
  model_save_dir: "models/trained/"
  # n_eval_episodes: number of episodes to evaluate agent on
  n_eval_episodes: 1
  # eval_freq: evaluate the agent every eval_freq train timesteps
  eval_freq: 128
  deterministic: True
stoptraining:
  ## settings related to creating stop training callback
  stop_threshhold_type: "succ"
  stop_reward_threshhold: 0.9
  stop_verbose: 1
custom_network:
  shared_layers: "64-64"
  policy_layer_sizes: "128-128"
  value_layer_sizes: "128-128"
  # can be "relu", "sigmoid" or "tanh"
  act_fn: "relu"
