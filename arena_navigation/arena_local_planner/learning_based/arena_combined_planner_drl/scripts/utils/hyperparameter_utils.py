from typing import Union

import argparse
from datetime import datetime as dt
import gym
import json
import os
import rosnode
import rospkg
import time
import warnings

from stable_baselines3 import PPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.utils import set_random_seed


from arena_navigation.arena_local_planner.learning_based.arena_local_planner_drl.rl_agent.envs.flatland_gym_env import (
    FlatlandEnv,
)
from arena_navigation.arena_local_planner.learning_based.arena_local_planner_drl.rl_agent.envs.flatland_gym_env import (
    FlatlandEnv,
)


""" 
Dict containing agent specific hyperparameter keys (for documentation and typing validation purposes)

:key agent_name: Precise agent name (as generated by get_agent_name())
:key robot: Robot name to load robot specific .yaml file containing settings
:key batch_size: Batch size (n_envs * n_steps)
:key gamma: Discount factor
:key n_steps: The number of steps to run for each environment per update
:key ent_coef: Entropy coefficient for the loss calculation
:key learning_rate: The learning rate, it can be a function
    of the current progress remaining (from 1 to 0)
    (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)
:key vf_coef: Value function coefficient for the loss calculation
:key max_grad_norm: The maximum value for the gradient clipping
:key gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator
:key m_batch_size: Minibatch size
:key n_epochs: Number of epoch when optimizing the surrogate loss
:key clip_range: Clipping parameter, it can be a function of the current progress
    remaining (from 1 to 0).
:key train_max_steps_per_episode: Max timesteps per training episode
:key eval_max_steps_per_episode: Max timesteps per evaluation episode
:key goal_radius: Radius of the goal
:key reward_fnc: Number of the reward function (defined in ../rl_agent/utils/reward.py)
:key discrete_action_space: If robot uses discrete action space
:key normalize: If observations are normalized before fed to the network
:key task_mode: Mode tasks will be generated in (custom, random, staged).
:key curr_stage: In case of staged training which stage to start with.
:param n_timesteps: The number of timesteps trained on in total.
"""
hyperparams = {
    key: None
    for key in [
        "agent_name",
        "robot",
        "batch_size",
        "gamma",
        "n_steps",
        "ent_coef",
        "learning_rate",
        "vf_coef",
        "max_grad_norm",
        "gae_lambda",
        "m_batch_size",
        "n_epochs",
        "clip_range",
        "reward_fnc",
        "discrete_action_space",
        "normalize",
        "task_mode",
        "curr_stage",
        "train_max_steps_per_episode",
        "eval_max_steps_per_episode",
        "goal_radius",
    ]
}

def _fill_dict(args: argparse.Namespace):
    params = dict()
    params["agent_name"] = args.agent_name
    params["robot"] = args.robot
    params["batch_size"] = args.batch_size
    params["gamma"] = args.gamma
    params["n_steps"] = args.n_steps
    params["ent_coef"] = args.ent_coef
    params["learning_rate"] = args.learning_rate
    params["vf_coef"] = args.vf_coef
    params["max_grad_norm"] = args.max_grad_norm
    params["gae_lambda"] = args.gae_lambda
    params["m_batch_size"] = args.mini_batch_size
    params["n_epochs"] = args.n_epochs
    params["clip_range"] = args.clip_range
    params["reward_fnc"] = args.reward_fnc
    params["discret_actions_space"] = False
    params["normalize"] = args.normalize
    params["task_mode"] = args.task_mode
    params["curr_stage"] = args.task_curr_stage
    params["train_max_steps_per_episode"] = args.train_max_steps_per_episode
    params["eval_max_steps_per_episode"] = args.eval_max_steps_per_episode
    params["goal_radius"] = args.goal_radius
    return params


def write_hyperparameters_json(args: argparse.Namespace, save_paths: dict) -> None:
    """
    Write hyperparameters.json to agent directory

    :param hyperparams: dict containing model specific hyperparameters
    :param PATHS: dictionary containing model specific paths
    """
    doc_location = os.path.join(save_paths.get("model"), "hyperparameters.json")

    hyperparams = _fill_dict(args)
    with open(doc_location, "w", encoding="utf-8") as target:
        json.dump(hyperparams, target, ensure_ascii=False, indent=4)


def load_hyperparameters_json(
    PATHS: dict, from_scratch: bool = False, config_name: str = "default"
) -> dict:
    """
    Load hyperparameters from model directory when loading - when training from scratch
    load from ../configs/hyperparameters

    :param PATHS: dictionary containing model specific paths
    :param from_scatch: if training from scratch
    :param config_name: file name of json file when training from scratch
    """
    if from_scratch:
        doc_location = os.path.join(PATHS.get("hyperparams"), config_name + ".json")
    else:
        doc_location = os.path.join(PATHS.get("model"), "hyperparameters.json")

    if os.path.isfile(doc_location):
        with open(doc_location, "r") as file:
            hyperparams = json.load(file)
        check_hyperparam_format(loaded_hyperparams=hyperparams, PATHS=PATHS)
        return hyperparams
    else:
        if from_scratch:
            raise FileNotFoundError(
                "Found no '%s.json' in %s" % (config_name, PATHS.get("hyperparams"))
            )
        else:
            raise FileNotFoundError(
                "Found no 'hyperparameters.json' in %s" % PATHS.get("model")
            )


def update_total_timesteps_json(timesteps: int, PATHS: dict) -> None:
    """
    Update total number of timesteps in json file

    :param hyperparams_obj(object, agent_hyperparams): object containing containing model specific hyperparameters
    :param PATHS: dictionary containing model specific paths
    """
    doc_location = os.path.join(PATHS.get("model"), "hyperparameters.json")
    hyperparams = load_hyperparameters_json(PATHS=PATHS)

    try:
        curr_timesteps = int(hyperparams["n_timesteps"]) + timesteps
        hyperparams["n_timesteps"] = curr_timesteps
    except Exception:
        raise Warning(
            "Parameter 'total_timesteps' not found or not of type Integer in 'hyperparameter.json'!"
        )
    else:
        with open(doc_location, "w", encoding="utf-8") as target:
            json.dump(hyperparams, target, ensure_ascii=False, indent=4)


def print_hyperparameters(hyperparams: dict) -> None:
    print("\n--------------------------------")
    print("         HYPERPARAMETERS         \n")
    for param, param_val in hyperparams.items():
        print("{:30s}{:<10s}".format((param + ":"), str(param_val)))
    print("--------------------------------\n\n")


def check_hyperparam_format(loaded_hyperparams: dict, PATHS: dict) -> None:
    if set(hyperparams.keys()) != set(loaded_hyperparams.keys()):
        missing_keys = set(hyperparams.keys()).difference(
            set(loaded_hyperparams.keys())
        )
        redundant_keys = set(loaded_hyperparams.keys()).difference(
            set(hyperparams.keys())
        )
        raise AssertionError(
            f"unmatching keys, following keys missing: {missing_keys} \n"
            f"following keys unused: {redundant_keys}"
        )
    if not isinstance(loaded_hyperparams["discrete_action_space"], bool):
        raise TypeError("Parameter 'discrete_action_space' not of type bool")
    if loaded_hyperparams["task_mode"] not in ["custom", "random", "staged"]:
        raise TypeError("Parameter 'task_mode' has unknown value")





def check_batch_size(n_envs: int, batch_size: int, mn_batch_size: int) -> None:
    assert (
        batch_size > mn_batch_size
    ), f"Mini batch size {mn_batch_size} is bigger than batch size {batch_size}"

    assert (
        batch_size % mn_batch_size == 0
    ), f"Batch size {batch_size} isn't divisible by mini batch size {mn_batch_size}"

    assert (
        batch_size % n_envs == 0
    ), f"Batch size {batch_size} isn't divisible by n_envs {n_envs}"

    assert (
        batch_size % mn_batch_size == 0
    ), f"Batch size {batch_size} isn't divisible by mini batch size {mn_batch_size}"
